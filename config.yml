run_id: "run_name"
# dataset
# dataset name, param1, param2
dataset: 20newsgroups

# embeddings
embedding: glove,50
# embedding aggregation method to document-level representations
# - avg: average word embeddings to a single doc vector
# - pad,NUM,filter: force NUM words / embeddings per document, padding / pruning where necessary 
# NUM words are selected by filter:
# first: just get the first NUM words
# freq: get the NUM words that are most frequent in the dataset
# aggregation: pad,10,first
aggregation: avg

# semantic resource settings
# resource,population_strategy,weighting_strategy,pruning_strategy

semantic_resource: wordnet

# what is considered as a semantic information
semantic_unit: synset

# the form of semantic information to use
# frequencies or tfidf
semantic_weights: freq
# drop synsets with dataset-wise freq less than this threshold
#semantic_freq_threshold: 15
# how the semantic information is combined with the textual one
enrichment: concat

# pos: get the first that matches the POS tag assigned
# embedding-centroid, num-threshold: select synset by min proximity to precomputed semantic embeddings
# params:
# num-threshold: min. number of context words for a synset 
semantic_disambiguation: context-embedding,50
semantic_embedding_aggregation: avg
semantic_word_context_file: wordnet_synset_examples_definitions.pickle

# learning model
# dnn or lstm
# learner: lstm,128,1,10
learner: mlp,128,4

train:
  epochs: 5
  folds: 3
  early_stopping_patience: -1
  batch_size: 20

log_level: debug
# log_level: info

#parallelization: 10
#parallelization: 4

options:
  data_limit: 100

results_folder: "results"
serialization_dir: "serialization"
